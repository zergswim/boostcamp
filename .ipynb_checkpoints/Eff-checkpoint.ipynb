{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50104687-b0cc-47e5-9728-828d8bd0d552",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/rwightman/efficientdet-pytorch\n",
    "#!cd ./efficientdet-pytorch && python setup.py install\n",
    "import effdet\n",
    "print(effdet)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3825baa-27f2-4b70-bbe4-8a7e11b3d921",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pycocotools>=2.0.2\n",
    "# !pip install timm>=0.3.2\n",
    "# !pip install omegaconf>=2.0\n",
    "# !pip install ensemble-boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7128a4e-6adb-466d-93c3-cd0b49d556d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "import os\n",
    "from datetime import datetime\n",
    "import time\n",
    "import random\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from glob import glob\n",
    "from collections import Counter\n",
    "\n",
    "from ensemble_boxes import weighted_boxes_fusion\n",
    "import albumentations as A\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from torch.utils.data.sampler import SequentialSampler, RandomSampler\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "\n",
    "from effdet import get_efficientdet_config, EfficientDet, DetBenchTrain\n",
    "from effdet.efficientdet import HeadNet\n",
    "from effdet import create_model, unwrap_bench, create_loader, create_dataset, create_evaluator, create_model_from_config\n",
    "from effdet.data import resolve_input_config, SkipSubset\n",
    "from effdet.anchors import Anchors, AnchorLabeler\n",
    "from timm.models import resume_checkpoint, load_checkpoint\n",
    "from timm.utils import *\n",
    "from timm.optim import create_optimizer\n",
    "from timm.scheduler import create_scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b375088e-e0bb-4d88-8d79-d37b385aa914",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "SEED = 42\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "seed_everything(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d6ab565-f839-4759-8f29-8fc35a9b36c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycocotools.coco import COCO\n",
    "coco = COCO('./dataset/train.json')\n",
    "\n",
    "train_df = pd.DataFrame()\n",
    "\n",
    "image_ids = []\n",
    "class_name = []\n",
    "class_id = []\n",
    "area = []\n",
    "x_min = []\n",
    "y_min = []\n",
    "x_max = []\n",
    "y_max = []\n",
    "classes = [\"General trash\", \"Paper\", \"Paper pack\", \"Metal\", \"Glass\", \n",
    "           \"Plastic\", \"Styrofoam\", \"Plastic bag\", \"Battery\", \"Clothing\"]\n",
    "for image_id in coco.getImgIds():\n",
    "        \n",
    "    image_info = coco.loadImgs(image_id)[0]\n",
    "    ann_ids = coco.getAnnIds(imgIds=image_info['id'])\n",
    "    anns = coco.loadAnns(ann_ids)\n",
    "        \n",
    "    file_name = image_info['file_name']\n",
    "        \n",
    "    for ann in anns:\n",
    "        image_ids.append(file_name)\n",
    "        class_name.append(classes[ann['category_id']])\n",
    "        class_id.append(ann['category_id'])\n",
    "        area.append(ann['area'])\n",
    "        x_min.append(float(ann['bbox'][0]))\n",
    "        y_min.append(float(ann['bbox'][1]))\n",
    "        x_max.append(float(ann['bbox'][0]) + float(ann['bbox'][2]))\n",
    "        y_max.append(float(ann['bbox'][1]) + float(ann['bbox'][3]))\n",
    "\n",
    "train_df['image_id'] = image_ids\n",
    "train_df['class_name'] = class_name\n",
    "train_df['class_id'] = class_id\n",
    "train_df['x_min'] = x_min\n",
    "train_df['y_min'] = y_min\n",
    "train_df['x_max'] = x_max\n",
    "train_df['y_max'] = y_max\n",
    "train_df['area'] = area\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879b172c-c316-47d9-98c5-b1e4c3ba2ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_s = train_df[train_df['area'] < 30000]\n",
    "train_df_b = train_df[train_df['area'] >= 30000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "512a8253-b000-49f3-835d-08a160a84074",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df_s.area.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407a35a3-5595-4dbb-814a-3298d676d26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df_b.area.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759f4451-bc5b-41a4-8241-a1e5d78d7460",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df.class_name.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c97596-ac65-43bf-aa30-6cbfdc380e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df_b.class_name.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e2c956-759a-44b6-96b0-6ce85e2a8d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df_s.class_name.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c18c6d1-6d5e-41f5-842e-0f76f4e24ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tdf = train_df_b\n",
    "# # sample = tdf[tdf['image_id'] == 'train/0001.jpg']\n",
    "# sample = train_df[train_df['image_id'] == 'train/4197.jpg']\n",
    "# # sample = tdf[tdf['image_id'] == 'train/4197.jpg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec35354-0937-4806-a840-81658af27386",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cv2\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# def showTBox(img, x1, y1, x2, y2, label):\n",
    "#     cv2.rectangle(img, (x1, y1), (x2, y2), (255, 0, 0), 2)\n",
    "\n",
    "#     y = y1 - 15 if y1 - 15 > 15 else y1 + 15\n",
    "#     # label = \"{}\".format(str(LABEL_NAME[label]))\n",
    "#     label = \"{}\".format(str(label))\n",
    "#     cv2.putText(img, label, (x1, y), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 2)\n",
    "    \n",
    "# img_path = './dataset/'+sample['image_id'].tolist()[0]\n",
    "# img = plt.imread(img_path)\n",
    "\n",
    "# sdict = sample.to_dict('records')\n",
    "# for s in sdict:\n",
    "#     showTBox(img, int(s['x_min']), int(s['y_min']), int(s['x_max']), int(s['y_max']), s['class_name'])\n",
    "\n",
    "# plt.figure(figsize=(8,8))\n",
    "# plt.imshow(img)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff46474e-5075-4499-b32e-9e580becd2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample = train_df[train_df['image_id'] == 'train/0946.jpg']\n",
    "# # sample = tdf[tdf['image_id'] == 'train/0946.jpg']\n",
    "# img_path = './dataset/'+sample['image_id'].tolist()[0]\n",
    "# img = plt.imread(img_path)\n",
    "\n",
    "# sdict = sample.to_dict('records')\n",
    "# for s in sdict:\n",
    "#     showTBox(img, int(s['x_min']), int(s['y_min']), int(s['x_max']), int(s['y_max']), s['class_name'])\n",
    "\n",
    "# plt.figure(figsize=(8,8))\n",
    "# plt.imshow(img)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f07237a-50fb-41f2-9985-61582bb428e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_annotations_wbf = train_df #train_df_b.reset_index(drop=True) #train_df\n",
    "\n",
    "print(train_df.index, df_annotations_wbf.index)\n",
    "# print(df_annotations_wbf.image_id.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c439b643-5ae7-4689-82e5-8a26182b1105",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMG_SIZE = 896\n",
    "IMG_SIZE = 1024\n",
    "\n",
    "class TrainGlobalConfig:\n",
    "    def __init__(self):\n",
    "        self.num_classes = 10\n",
    "        self.num_workers = 2\n",
    "        # self.batch_size = 16\n",
    "        self.batch_size = 4 #6\n",
    "        # self.batch_size = 1 #16\n",
    "        self.n_epochs = 1000\n",
    "        self.lr = 0.0002\n",
    "        # self.model_name = 'tf_efficientdet_d1'\n",
    "        self.model_name = 'tf_efficientdet_d4_ap'\n",
    "        # self.model_name = 'tf_efficientdet_d3_ap'\n",
    "        self.folder = 'training_job'\n",
    "        self.verbose = True\n",
    "        self.verbose_step = 1\n",
    "        self.step_scheduler = True\n",
    "        self.validation_scheduler = False\n",
    "        self.n_img_count = len(df_annotations_wbf.image_id.unique())\n",
    "        # self.n_img_count = len(train_df.image_id.unique())\n",
    "        self.OptimizerClass = torch.optim.AdamW\n",
    "        self.SchedulerClass = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts\n",
    "        self.scheduler_params = dict(\n",
    "                            T_0=50,\n",
    "                            T_mult=1,\n",
    "                            eta_min=0.0001,\n",
    "                            last_epoch=-1,\n",
    "                            verbose=False\n",
    "                            )\n",
    "        self.kfold = 5\n",
    "    \n",
    "    def reset(self):\n",
    "        self.OptimizerClass = torch.optim.AdamW\n",
    "        self.SchedulerClass = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts\n",
    "\n",
    "train_config = TrainGlobalConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "946e3820-e282-4283-a3ca-ae1bfa62ef2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GroupKFold, train_test_split, StratifiedKFold\n",
    "\n",
    "df_annotations_wbf['fold'] = -1\n",
    "# group_kfold  = GroupKFold(n_splits = 3)\n",
    "# for fold, (train_index, val_index) in enumerate(group_kfold.split(df_annotations_wbf, groups=df_annotations_wbf.image_id.tolist())):\n",
    "\n",
    "X = df_annotations_wbf\n",
    "y = df_annotations_wbf.class_id\n",
    "\n",
    "kfold  = StratifiedKFold(n_splits = 5)\n",
    "for fold, (train_index, val_index) in enumerate(kfold.split(X, y)):\n",
    "    df_annotations_wbf.loc[val_index, 'fold'] = fold\n",
    "df_annotations_wbf.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6def7053-4c72-44be-9b88-6cefbf457710",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN_ROOT_PATH = '../input/vinbigdata-original-image-dataset/vinbigdata/train'\n",
    "TRAIN_ROOT_PATH = './dataset/'\n",
    "\n",
    "class DatasetRetriever(Dataset):\n",
    "\n",
    "    def __init__(self, marking, image_ids, transforms=None, test=False):\n",
    "        super().__init__()\n",
    "        self.image_ids = image_ids\n",
    "        self.marking = marking\n",
    "        self.transforms = transforms\n",
    "        self.test = test\n",
    "        \n",
    "    def __getitem__(self, index: int):\n",
    "        image_id = self.image_ids[index]\n",
    "        \n",
    "        # image, boxes, labels = self.load_image_and_boxes(index)\n",
    "        # print(type(image), type(boxes), type(labels), random.random())\n",
    "\n",
    "        if self.test or random.random() > 0.5:\n",
    "            image, boxes, labels = self.load_image_and_boxes(index)\n",
    "        else:\n",
    "            image, boxes, labels = self.load_cutmix_image_and_boxes(index)\n",
    "        \n",
    "        # if self.test or random.random() > 0.33:\n",
    "        #     image, boxes, labels = self.load_image_and_boxes(index)\n",
    "        # elif random.random() > 0.5:\n",
    "        #     image, boxes, labels = self.load_cutmix_image_and_boxes(index)\n",
    "        # else:\n",
    "        #     image, boxes, labels = self.load_mixup_image_and_boxes(index)\n",
    "        \n",
    "        ## To prevent ValueError: y_max is less than or equal to y_min for bbox from albumentations bbox_utils\n",
    "        labels = np.array(labels, dtype=np.int).reshape(len(labels), 1)\n",
    "        combined = np.hstack((boxes.astype(np.int), labels))\n",
    "        combined = combined[np.logical_and(combined[:,2] > combined[:,0],\n",
    "                                                          combined[:,3] > combined[:,1])]\n",
    "        boxes = combined[:, :4]\n",
    "        labels = combined[:, 4].tolist()\n",
    "        \n",
    "        target = {}\n",
    "        target['boxes'] = boxes\n",
    "        target['labels'] = torch.tensor(labels)\n",
    "        target['image_id'] = torch.tensor([index])\n",
    "        if self.transforms:\n",
    "            for i in range(10):\n",
    "                sample = self.transforms(**{\n",
    "                    'image': image,\n",
    "                    'bboxes': target['boxes'],\n",
    "                    'labels': labels\n",
    "                })\n",
    "                if len(sample['bboxes']) > 0:\n",
    "                    image = sample['image']\n",
    "                    target['boxes'] = torch.stack(tuple(map(torch.tensor, zip(*sample['bboxes'])))).permute(1, 0)\n",
    "                    target['boxes'][:,[0,1,2,3]] = target['boxes'][:,[1,0,3,2]]  ## ymin, xmin, ymax, xmax\n",
    "                    break\n",
    "            \n",
    "            ## Handling case where no valid bboxes are present\n",
    "            if len(target['boxes'])==0 or i==9:\n",
    "                return None\n",
    "            else:\n",
    "                ## Handling case where augmentation and tensor conversion yields no valid annotations\n",
    "                try:\n",
    "                    assert torch.is_tensor(image), f\"Invalid image type:{type(image)}\"\n",
    "                    assert torch.is_tensor(target['boxes']), f\"Invalid target type:{type(target['boxes'])}\"\n",
    "                except Exception as E:\n",
    "                    print(\"Image skipped:\", E)\n",
    "                    return None      \n",
    "\n",
    "        return image, target, image_id\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.image_ids.shape[0]\n",
    "    \n",
    "    def load_image_and_boxes(self, index):\n",
    "        image_id = self.image_ids[index]\n",
    "#         print(f'{TRAIN_ROOT_PATH}/{image_id}.jpg')\n",
    "        # image = cv2.imread(f'{TRAIN_ROOT_PATH}/{image_id}.jpg', cv2.IMREAD_COLOR).copy()\n",
    "        image = cv2.imread(f'{TRAIN_ROOT_PATH}{image_id}', cv2.IMREAD_COLOR).copy()\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
    "        image /= 255.0\n",
    "        records = self.marking[self.marking['image_id'] == image_id]\n",
    "        boxes = records[['x_min', 'y_min', 'x_max', 'y_max']].values\n",
    "        labels = records['class_id'].tolist()\n",
    "        resize_transform = A.Compose(\n",
    "                                    [A.Resize(height=IMG_SIZE, width=IMG_SIZE, p=1.0)], \n",
    "                                    # [A.Resize(height=512, width=512, p=1.0)], \n",
    "                                    p=1.0, \n",
    "                                    bbox_params=A.BboxParams(\n",
    "                                        format='pascal_voc',\n",
    "                                        min_area=0.1, \n",
    "                                        min_visibility=0.1,\n",
    "                                        label_fields=['labels'])\n",
    "                                    )\n",
    "\n",
    "        resized = resize_transform(**{\n",
    "                'image': image,\n",
    "                'bboxes': boxes,\n",
    "                'labels': labels\n",
    "            })\n",
    "\n",
    "        resized_bboxes = np.vstack((list(bx) for bx in resized['bboxes']))\n",
    "        return resized['image'], resized_bboxes, resized['labels']\n",
    "    \n",
    "    def load_mixup_image_and_boxes(self, index):\n",
    "        image, boxes, labels = self.load_image_and_boxes(index)\n",
    "        r_image, r_boxes, r_labels = self.load_image_and_boxes(random.randint(0, self.image_ids.shape[0] - 1))\n",
    "        return (image+r_image)/2, np.vstack((boxes, r_boxes)).astype(np.int32), np.concatenate((labels, r_labels))\n",
    "\n",
    "    # def load_cutmix_image_and_boxes(self, index, imsize=512):\n",
    "    def load_cutmix_image_and_boxes(self, index, imsize=IMG_SIZE):\n",
    "        \"\"\" \n",
    "        This implementation of cutmix author:  https://www.kaggle.com/nvnnghia \n",
    "        Refactoring and adaptation: https://www.kaggle.com/shonenkov\n",
    "        \"\"\"\n",
    "        w, h = imsize, imsize\n",
    "        s = imsize // 2\n",
    "    \n",
    "        xc, yc = [int(random.uniform(imsize * 0.25, imsize * 0.75)) for _ in range(2)]  # center x, y\n",
    "        indexes = [index] + [random.randint(0, self.image_ids.shape[0] - 1) for _ in range(3)]\n",
    "\n",
    "        result_image = np.full((imsize, imsize, 3), 1, dtype=np.float32)\n",
    "        result_boxes = []\n",
    "        result_labels = np.array([], dtype=np.int)\n",
    "\n",
    "        for i, index in enumerate(indexes):\n",
    "            image, boxes, labels = self.load_image_and_boxes(index)\n",
    "            if i == 0:\n",
    "                x1a, y1a, x2a, y2a = max(xc - w, 0), max(yc - h, 0), xc, yc  # xmin, ymin, xmax, ymax (large image)\n",
    "                x1b, y1b, x2b, y2b = w - (x2a - x1a), h - (y2a - y1a), w, h  # xmin, ymin, xmax, ymax (small image)\n",
    "            elif i == 1:  # top right\n",
    "                x1a, y1a, x2a, y2a = xc, max(yc - h, 0), min(xc + w, s * 2), yc\n",
    "                x1b, y1b, x2b, y2b = 0, h - (y2a - y1a), min(w, x2a - x1a), h\n",
    "            elif i == 2:  # bottom left\n",
    "                x1a, y1a, x2a, y2a = max(xc - w, 0), yc, xc, min(s * 2, yc + h)\n",
    "                x1b, y1b, x2b, y2b = w - (x2a - x1a), 0, max(xc, w), min(y2a - y1a, h)\n",
    "            elif i == 3:  # bottom right\n",
    "                x1a, y1a, x2a, y2a = xc, yc, min(xc + w, s * 2), min(s * 2, yc + h)\n",
    "                x1b, y1b, x2b, y2b = 0, 0, min(w, x2a - x1a), min(y2a - y1a, h)\n",
    "            result_image[y1a:y2a, x1a:x2a] = image[y1b:y2b, x1b:x2b]\n",
    "            padw = x1a - x1b\n",
    "            padh = y1a - y1b\n",
    "\n",
    "            boxes[:, 0] += padw\n",
    "            boxes[:, 1] += padh\n",
    "            boxes[:, 2] += padw\n",
    "            boxes[:, 3] += padh\n",
    "\n",
    "            result_boxes.append(boxes)\n",
    "            result_labels = np.concatenate((result_labels, labels))\n",
    "\n",
    "        result_boxes = np.concatenate(result_boxes, 0)\n",
    "        np.clip(result_boxes[:, 0:], 0, 2 * s, out=result_boxes[:, 0:])\n",
    "        result_boxes = result_boxes.astype(np.int32)\n",
    "        index_to_use = np.where((result_boxes[:,2]-result_boxes[:,0])*(result_boxes[:,3]-result_boxes[:,1]) > 0)\n",
    "        result_boxes = result_boxes[index_to_use]\n",
    "        result_labels = result_labels[index_to_use]\n",
    "        \n",
    "        return result_image, result_boxes, result_labels\n",
    "    \n",
    "# train_dataset = DatasetRetriever(image_ids=train_ids, marking=df_annotations_wbf, transforms=get_train_transforms(), test=False)\n",
    "# next(iter(train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b5b95a-0aab-4328-b895-d46d98800f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_transforms():\n",
    "    return A.Compose(\n",
    "        [\n",
    "        ## RandomSizedCrop not working for some reason. I'll post a thread for this issue soon.\n",
    "        ## Any help or suggestions are appreciated.\n",
    "#         A.RandomSizedCrop(min_max_height=(300, 512), height=512, width=512, p=0.5),\n",
    "#         A.RandomSizedCrop(min_max_height=(300, 1000), height=1000, width=1000, p=0.5),\n",
    "        # A.OneOf([\n",
    "        #     A.HueSaturationValue(hue_shift_limit=0.2, sat_shift_limit= 0.2, \n",
    "        #                          val_shift_limit=0.2, p=0.9),\n",
    "        #     A.RandomBrightnessContrast(brightness_limit=0.2, \n",
    "        #                                contrast_limit=0.2, p=0.9),\n",
    "        # ],p=0.9),\n",
    "        # A.JpegCompression(quality_lower=85, quality_upper=95, p=0.2),\n",
    "        # A.OneOf([\n",
    "        #     A.Blur(blur_limit=3, p=1.0),\n",
    "        #     A.MedianBlur(blur_limit=3, p=1.0)\n",
    "        #     ],p=0.1),\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.VerticalFlip(p=0.5),\n",
    "        A.RandomRotate90(p=0.5),\n",
    "        # A.Transpose(p=0.5),\n",
    "        A.Resize(height=IMG_SIZE, width=IMG_SIZE, p=1),\n",
    "        # A.Resize(height=512, width=512, p=1),\n",
    "        A.Cutout(num_holes=8, max_h_size=64, max_w_size=64, fill_value=0, p=0.5),\n",
    "        ToTensorV2(p=1.0)\n",
    "        ], \n",
    "        p=1.0, \n",
    "        bbox_params=A.BboxParams(\n",
    "            format='pascal_voc',\n",
    "            min_area=0, \n",
    "            min_visibility=0,\n",
    "            label_fields=['labels']\n",
    "        )\n",
    "    )\n",
    "\n",
    "def get_valid_transforms():\n",
    "    return A.Compose(\n",
    "        [\n",
    "            A.Resize(height=IMG_SIZE, width=IMG_SIZE, p=1.0),\n",
    "            # A.Resize(height=512, width=512, p=1.0),\n",
    "            ToTensorV2(p=1.0),\n",
    "        ], \n",
    "        p=1.0, \n",
    "        bbox_params=A.BboxParams(\n",
    "            format='pascal_voc',\n",
    "            min_area=0, \n",
    "            min_visibility=0,\n",
    "            label_fields=['labels']\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01dedfc-b560-42e4-8894-b5d7e82b8256",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Fitter:\n",
    "    def __init__(self, model, device, config):\n",
    "        self.config = config\n",
    "        self.epoch = 0\n",
    "\n",
    "        self.base_dir = f'./{config.folder}'\n",
    "        \n",
    "        if not os.path.exists(self.base_dir):\n",
    "            os.makedirs(self.base_dir)\n",
    "        \n",
    "        self.log_path = f'{self.base_dir}/log.txt'\n",
    "        self.best_summary_loss = 10**5\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "\n",
    "        param_optimizer = list(self.model.named_parameters())\n",
    "        no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "        optimizer_grouped_parameters = [\n",
    "            {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.001},\n",
    "            {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "        ] \n",
    "\n",
    "        self.optimizer = config.OptimizerClass(self.model.parameters(), lr=config.lr)\n",
    "        self.scheduler = config.SchedulerClass(self.optimizer, **config.scheduler_params)\n",
    "        self.log(f'Fitter prepared. Device is {self.device}')\n",
    "\n",
    "    def fit(self, train_loader, validation_loader, fold):\n",
    "        history_dict = {}\n",
    "        history_dict['epoch'] = []\n",
    "        history_dict['train_loss'] = []\n",
    "        history_dict['val_loss'] = []\n",
    "        history_dict['train_lr'] = []\n",
    "        \n",
    "        for e in range(self.config.n_epochs):\n",
    "            history_dict['epoch'].append(self.epoch)\n",
    "            lr = self.optimizer.param_groups[0]['lr']\n",
    "            timestamp = datetime.utcnow().isoformat()\n",
    "            \n",
    "            if self.config.verbose:\n",
    "                self.log(f'\\n{timestamp}\\nLR: {lr}')\n",
    "\n",
    "            t = time.time()\n",
    "            summary_loss, loss_trend, lr_trend = self.train_epoch(train_loader)\n",
    "            history_dict['train_loss'].append(loss_trend)\n",
    "            history_dict['train_lr'].append(lr_trend)\n",
    "            self.log(f'[RESULT]: Train. Epoch: {self.epoch}, summary_loss: {summary_loss.avg:.5f}, time: {(time.time() - t):.5f}')\n",
    "            self.save(f'{self.base_dir}/{fold}-{IMG_SIZE}-last-checkpoint.bin')\n",
    "            \n",
    "            t = time.time()\n",
    "            summary_loss, loss_trend = self.validation(validation_loader)\n",
    "            history_dict['val_loss'].append(loss_trend)\n",
    "            self.log(f'[RESULT]: Val. Epoch: {self.epoch}, summary_loss: {summary_loss.avg:.5f}, time: {(time.time() - t):.5f}')\n",
    "            \n",
    "            if summary_loss.avg < self.best_summary_loss:\n",
    "                self.best_summary_loss = summary_loss.avg\n",
    "                self.model.eval()\n",
    "                self.save(f'{self.base_dir}/{fold}-{IMG_SIZE}-best-checkpoint-{str(self.epoch).zfill(3)}epoch.bin')\n",
    "                \n",
    "                try:\n",
    "                    os.remove(f)\n",
    "                except:pass\n",
    "                f = f'{self.base_dir}/{fold}-{IMG_SIZE}-best-checkpoint-{str(self.epoch).zfill(3)}epoch.bin'\n",
    "\n",
    "            if self.config.validation_scheduler:\n",
    "                self.scheduler.step(metrics=summary_loss.avg)\n",
    "\n",
    "            self.epoch += 1\n",
    "        return history_dict\n",
    "\n",
    "    def train_epoch(self, train_loader):\n",
    "        self.model.train()\n",
    "        summary_loss = AverageMeter()\n",
    "        t = time.time()\n",
    "        loss_trend = []\n",
    "        lr_trend = []\n",
    "        for step, (images, targets, image_ids) in tqdm(enumerate(train_loader), total=len(train_loader)):\n",
    "            if self.config.verbose:\n",
    "                if step % self.config.verbose_step == 0:\n",
    "                    print(\n",
    "                        f'Train Step {step}/{len(train_loader)}, ' + \\\n",
    "                        f'summary_loss: {summary_loss.avg:.5f}, ' + \\\n",
    "                        f'time: {(time.time() - t):.5f}', end='\\r'\n",
    "                    )            \n",
    "            \n",
    "            images = torch.stack(images)\n",
    "            images = images.to(self.device).float()\n",
    "            \n",
    "            target_res = {}\n",
    "            boxes = [target['boxes'].to(self.device).float() for target in targets]\n",
    "            labels = [target['labels'].to(self.device).float() for target in targets]\n",
    "            target_res['bbox'] = boxes\n",
    "            target_res['cls'] = labels\n",
    "            self.optimizer.zero_grad()\n",
    "            output = self.model(images, target_res)\n",
    "\n",
    "            loss = output['loss']\n",
    "            loss.backward()\n",
    "            summary_loss.update(loss.detach().item(), self.config.batch_size)\n",
    "            self.optimizer.step()\n",
    "\n",
    "            if self.config.step_scheduler:\n",
    "                self.scheduler.step()\n",
    "\n",
    "            \n",
    "            lr = self.optimizer.param_groups[0]['lr']\n",
    "            loss_trend.append(summary_loss.avg)\n",
    "            lr_trend.append(lr)\n",
    "        return summary_loss, loss_trend, lr_trend\n",
    "    \n",
    "    def validation(self, val_loader):\n",
    "        self.model.eval()\n",
    "        summary_loss = AverageMeter()\n",
    "        t = time.time()\n",
    "        loss_trend = []\n",
    "#         lr_trend = []\n",
    "        \n",
    "        for step, (images, targets, image_ids) in tqdm(enumerate(val_loader), total=len(val_loader)):\n",
    "            if self.config.verbose:\n",
    "                if step % self.config.verbose_step == 0:\n",
    "                    print(\n",
    "                        f'Val Step {step}/{len(val_loader)}, ' + \\\n",
    "                        f'summary_loss: {summary_loss.avg:.5f}, ' + \\\n",
    "                        f'time: {(time.time() - t):.5f}', end='\\r'\n",
    "                    )\n",
    "            with torch.no_grad():\n",
    "                images = torch.stack(images)\n",
    "                images = images.to(self.device).float()\n",
    "                target_res = {}\n",
    "                boxes = [target['boxes'].to(self.device).float() for target in targets]\n",
    "                labels = [target['labels'].to(self.device).float() for target in targets]\n",
    "                target_res['bbox'] = boxes\n",
    "                target_res['cls'] = labels \n",
    "                target_res[\"img_scale\"] = torch.tensor([1.0] * self.config.batch_size,\n",
    "                                                       dtype=torch.float).to(self.device)\n",
    "                target_res[\"img_size\"] = torch.tensor([images[0].shape[-2:]] * self.config.batch_size,\n",
    "                                                      dtype=torch.float).to(self.device)\n",
    "                \n",
    "                output = self.model(images, target_res)\n",
    "            \n",
    "                loss = output['loss']\n",
    "                summary_loss.update(loss.detach().item(), self.config.batch_size)\n",
    "\n",
    "                loss_trend.append(summary_loss.avg)\n",
    "        return summary_loss, loss_trend[-1]\n",
    "    \n",
    "    \n",
    "    def save(self, path):\n",
    "        self.model.eval()\n",
    "        torch.save({\n",
    "            'model_state_dict': self.model.model.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "            'scheduler_state_dict': self.scheduler.state_dict(),\n",
    "            'best_summary_loss': self.best_summary_loss,\n",
    "            'epoch': self.epoch,\n",
    "        }, path)\n",
    "\n",
    "    def load(self, path):\n",
    "        checkpoint = torch.load(path)\n",
    "        self.model.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "        self.best_summary_loss = checkpoint['best_summary_loss']\n",
    "        self.epoch = checkpoint['epoch'] + 1\n",
    "        print('epoch:', self.epoch, 'best_summary_loss:',self.best_summary_loss)\n",
    "        return self.model\n",
    "        \n",
    "    def log(self, message):\n",
    "        if self.config.verbose:\n",
    "            print(message)\n",
    "        with open(self.log_path, 'a+') as logger:\n",
    "            logger.write(f'{message}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ba0850-9c01-4a17-8d9f-b5d1395b570d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Training will resume if the checkpoint path is specified below\n",
    "checkpoint_path = None #'last-checkpoint.bin'\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else \"cpu\")\n",
    "# device = \"cpu\"\n",
    "\n",
    "## Filters out invalid return items from the Dataloader\n",
    "# def collate_fn(batch):\n",
    "#     batch = list(filter(lambda x : x is not None, batch))\n",
    "#     return default_collate(batch)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    batch = list(filter(lambda x: x is not None, batch))\n",
    "    \n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "# def collate_fn(batch):\n",
    "#     return tuple(zip(*batch))\n",
    "\n",
    "fold_history = []\n",
    "for val_fold in range(train_config.kfold):\n",
    "    print(f'Fold {val_fold+1}/{train_config.kfold}')\n",
    "    \n",
    "    train_ids = df_annotations_wbf[df_annotations_wbf['fold'] != val_fold].image_id.unique()\n",
    "    val_ids = df_annotations_wbf[df_annotations_wbf['fold'] == val_fold].image_id.unique()\n",
    "    \n",
    "    train_dataset = DatasetRetriever(\n",
    "                        image_ids=train_ids,\n",
    "                        marking=df_annotations_wbf,\n",
    "                        transforms=get_train_transforms(),\n",
    "                        test=False,\n",
    "                        )\n",
    "\n",
    "    validation_dataset = DatasetRetriever(\n",
    "                            image_ids=val_ids,\n",
    "                            marking=df_annotations_wbf,\n",
    "                            transforms=get_valid_transforms(),\n",
    "                            test=True,\n",
    "                            )\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=train_config.batch_size,\n",
    "        sampler=RandomSampler(train_dataset),\n",
    "        pin_memory=False,\n",
    "        drop_last=True,\n",
    "        num_workers=train_config.num_workers,\n",
    "        collate_fn=collate_fn,\n",
    "    )\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        validation_dataset, \n",
    "        batch_size=train_config.batch_size,\n",
    "        num_workers=train_config.num_workers,\n",
    "        shuffle=False,\n",
    "        sampler=SequentialSampler(validation_dataset),\n",
    "        pin_memory=False,\n",
    "        collate_fn=collate_fn,\n",
    "    )\n",
    "    \n",
    "    base_config = get_efficientdet_config(train_config.model_name)\n",
    "    base_config.image_size = (IMG_SIZE, IMG_SIZE) #(512, 512)\n",
    "\n",
    "    if(checkpoint_path):\n",
    "        print(f'Resuming from checkpoint: {checkpoint_path}')        \n",
    "        model = create_model_from_config(base_config, bench_task='train', bench_labeler=True,\n",
    "                                 num_classes=train_config.num_classes,\n",
    "                                 pretrained=False)\n",
    "        model.to(device)\n",
    "        \n",
    "        fitter = Fitter(model=model, device=device, config=train_config)\n",
    "        fitter.load(f\"./training_job/{val_fold+1}-{IMG_SIZE}-{checkpoint_path}\")\n",
    "    \n",
    "    else:\n",
    "        model = create_model_from_config(base_config, bench_task='train', bench_labeler=True,\n",
    "                                     pretrained=True,\n",
    "                                     num_classes=train_config.num_classes)\n",
    "        model.to(device)\n",
    "    \n",
    "        fitter = Fitter(model=model, device=device, config=train_config)  \n",
    "        \n",
    "    model_config = model.config\n",
    "    \n",
    "    # ###for train\n",
    "    # history_dict = fitter.fit(train_loader, val_loader, val_fold+1)\n",
    "    # fold_history.append(history_dict)\n",
    "    \n",
    "    ## Reset Optimizer and LR Sch+)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24cae62f-a503-4293-80e8-67f79675979a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# load ground truth\n",
    "# with open('./dataset/train.json', 'r') as outfile:\n",
    "with open('./dataset/test.json', 'r') as outfile:\n",
    "    train = (json.load(outfile))\n",
    "    \n",
    "# print(train['images'][0]['file_name'], train['annotations'][0]['bbox'], train['annotations'][0]['category_id'])\n",
    "\n",
    "file_name = train['images'][1]['file_name']\n",
    "img_id = train['images'][1]['id']\n",
    "img = plt.imread('./dataset/'+file_name)\n",
    "plt.imshow(img)\n",
    "plt.show()\n",
    "\n",
    "print(img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f901d45-d315-47a6-adeb-955f6176d4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(base_config)\n",
    "from PIL import Image\n",
    "\n",
    "bench = create_model_from_config(base_config, bench_task='predict',\n",
    "                             num_classes=train_config.num_classes)\n",
    "\n",
    "fitter = Fitter(model=bench, device=device, config=train_config)\n",
    "# bench = fitter.load('./training_job/last-checkpoint.bin')\n",
    "bench = fitter.load(f'./training_job/1-{IMG_SIZE}-last-checkpoint.bin')\n",
    "\n",
    "# bench.to(device)\n",
    "bench.eval()\n",
    "\n",
    "from torchvision.models import detection\n",
    "from torchvision import datasets, models, transforms\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "preprocess = transforms.Compose([\n",
    "    ## Resize는 사용하지 않고 원본을 추출\n",
    "   transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "   transforms.ToTensor(),\n",
    "   # normalize\n",
    "])\n",
    "\n",
    "# import albumentations as A\n",
    "# from albumentations.pytorch import ToTensorV2\n",
    "# preprocess = A.Compose([\n",
    "#         A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225), max_pixel_value=255.0, always_apply=False, p=1.0),\n",
    "#         ToTensorV2(p=1.0)\n",
    "# ])\n",
    "\n",
    "PIL_image = Image.fromarray(img)\n",
    "img_input = preprocess(PIL_image)\n",
    "print(img_input.unsqueeze(0).shape)\n",
    "\n",
    "images = img_input.unsqueeze(0)\n",
    "print(type(images), images.shape, images)\n",
    "outputs = bench(images).detach().cpu().numpy().copy().squeeze()\n",
    "print(outputs[0])\n",
    "# detections = model(img_input.unsqueeze(0))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a0b004-a875-4ce3-8336-f6e624d2abdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "LABEL_NAME = [\"General trash\", \"Paper\", \"Paper pack\", \"Metal\", \n",
    "              \"Glass\", \"Plastic\", \"Styrofoam\", \"Plastic bag\", \"Battery\", \"Clothing\"]\n",
    "\n",
    "def showBox(outputs, img):\n",
    "    for x1, y1, x2, y2, score, label in outputs:\n",
    "        if score > 0.2: #args[\"confidence\"]:\n",
    "            idx = int(label)\n",
    "            print(x1, y1, x2, y2, score, LABEL_NAME[idx], idx)\n",
    "            x1 = int(x1)\n",
    "            y1 = int(y1)\n",
    "            x2 = int(x2)\n",
    "            y2 = int(y2)\n",
    "            label = \"{}: {:.2f}%\".format(LABEL_NAME[idx], score * 100)\n",
    "            cv2.rectangle(img, (x1, y1), (x2, y2), (255, 0, 0), 1)\n",
    "            y = y1 - 15 if y1 - 15 > 15 else y1 + 15\n",
    "            cv2.putText(img, label, (x1, y), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 1)\n",
    "    plt.figure(figsize=(8,8))\n",
    "    plt.imshow(img)\n",
    "    plt.show()\n",
    "    \n",
    "# img = img_input.numpy().transpose((1, 2, 0)).copy()\n",
    "# print(img.shape)\n",
    "# showBox(outputs, img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98875f52-9cb1-4f44-81b4-710db98c953c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#inference\n",
    "\n",
    "# for tr in train['images']:\n",
    "#     file_name = tr['file_name']\n",
    "#     img_id = tr['id']\n",
    "#     print(file_name, img_id)\n",
    "#     break\n",
    "    # img = plt.imread('./dataset/'+file_name)\n",
    "\n",
    "from pycocotools.coco import COCO\n",
    "coco = COCO('./dataset/test.json')\n",
    "\n",
    "test_df = pd.DataFrame()\n",
    "\n",
    "image_ids = []\n",
    "class_name = []\n",
    "class_id = []\n",
    "area = []\n",
    "x_min = []\n",
    "y_min = []\n",
    "x_max = []\n",
    "y_max = []\n",
    "classes = [\"General trash\", \"Paper\", \"Paper pack\", \"Metal\", \"Glass\", \n",
    "           \"Plastic\", \"Styrofoam\", \"Plastic bag\", \"Battery\", \"Clothing\"]\n",
    "for image_id in coco.getImgIds():\n",
    "    image_info = coco.loadImgs(image_id)[0]\n",
    "    file_name = image_info['file_name']\n",
    "        \n",
    "    image_ids.append(file_name)\n",
    "    class_name.append('')\n",
    "    class_id.append(0)\n",
    "    x_min.append(0.)\n",
    "    y_min.append(0.)\n",
    "    x_max.append(10.)\n",
    "    y_max.append(10.)\n",
    "\n",
    "test_df['image_id'] = image_ids\n",
    "test_df['class_name'] = class_name\n",
    "test_df['class_id'] = class_id\n",
    "test_df['x_min'] = x_min\n",
    "test_df['y_min'] = y_min\n",
    "test_df['x_max'] = x_max\n",
    "test_df['y_max'] = y_max\n",
    "\n",
    "# test_df.to_csv(f'./test_df.csv', index=None)\n",
    "# print(test_df.shape, test_df.image_id.shape)\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb36ee7-eb1d-4ed3-b8e7-301344668049",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_dataset = DatasetRetriever(\n",
    "                        image_ids=test_df.image_id,\n",
    "                        marking=test_df,\n",
    "                        transforms=get_valid_transforms(),\n",
    "                        test=True,\n",
    "                        )\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset, \n",
    "    batch_size=train_config.batch_size,\n",
    "    num_workers=0, #train_config.num_workers,\n",
    "    shuffle=False,\n",
    "    sampler=SequentialSampler(test_dataset),\n",
    "    pin_memory=False,\n",
    "    collate_fn=collate_fn,\n",
    ")\n",
    "\n",
    "# device = \"cuda:0\"\n",
    "def submit(test_loader, check_point, save_filename):\n",
    "    bench_model = create_model_from_config(base_config, bench_task='predict', num_classes=train_config.num_classes)\n",
    "    fitter = Fitter(model=bench_model, device=device, config=train_config)\n",
    "    bench = fitter.load(check_point)\n",
    "    # bench = fitter.load('./training_job/1-last-checkpoint.bin')\n",
    "    bench.to(device)\n",
    "    bench.eval()\n",
    "    prediction_strings = []\n",
    "    file_names = []\n",
    "    \n",
    "    for step, (images, targets, image_ids) in tqdm(enumerate(test_loader), total=len(test_loader)):\n",
    "        with torch.no_grad():\n",
    "            images = torch.stack(images)\n",
    "            images = images.to(device).float()\n",
    "            result = bench(images).to(device)\n",
    "            \n",
    "            outputs = result.detach().cpu().numpy().copy().squeeze()\n",
    "\n",
    "            for idx, outs in enumerate(outputs):\n",
    "                prediction_string = \"\"\n",
    "                for x1, y1, x2, y2, score, label in outs:\n",
    "                    # print(x1, y1, x2, y2, score, label)\n",
    "                    if score > 0.05:\n",
    "                        prediction_string += \"{} {} {} {} {} {} \".format(str(int(label)), str(score), str(x1*1024/IMG_SIZE), str(y1*1024/IMG_SIZE), str(x2*1024/IMG_SIZE), str(y2*1024/IMG_SIZE))\n",
    "                        # prediction_string += \"{} {} {} {} {} {} \".format(str(int(label)), str(score), str(x1*2), str(y1*2), str(x2*2), str(y2*2))\n",
    "\n",
    "                prediction_strings.append(prediction_string)\n",
    "                file_names.append(image_ids[idx])\n",
    "\n",
    "    submission = pd.DataFrame()\n",
    "    submission['PredictionString'] = prediction_strings\n",
    "    submission['image_id'] = file_names\n",
    "    submission.to_csv(f'./{save_filename}', index=None)\n",
    "    print(submission.head())\n",
    "\n",
    "# submit(test_loader, f'./training_job/1-{IMG_SIZE}-last-checkpoint.bin', f'eff_kfold1_{IMG_SIZE}_e57_nocutoff.csv')\n",
    "submit(test_loader, f'./training_job/1-{IMG_SIZE}-last-checkpoint.bin', f'eff_kfold1_{IMG_SIZE}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13abe3e5-cfec-4106-b6c6-db719db5f007",
   "metadata": {},
   "outputs": [],
   "source": [
    "submit(test_loader, f'./training_job/2-{IMG_SIZE}-last-checkpoint.bin', f'eff_kfold2_{IMG_SIZE}.csv')\n",
    "submit(test_loader, f'./training_job/3-{IMG_SIZE}-last-checkpoint.bin', f'eff_kfold3_{IMG_SIZE}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c04487-4fd1-48f9-a458-2efe92db5212",
   "metadata": {},
   "outputs": [],
   "source": [
    "submit(test_loader, f'./training_job/4-{IMG_SIZE}-last-checkpoint.bin', f'eff_kfold4_{IMG_SIZE}.csv')\n",
    "submit(test_loader, f'./training_job/5-{IMG_SIZE}-last-checkpoint.bin', f'eff_kfold5_{IMG_SIZE}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba322f32-583b-4e46-8e36-1e089ea0671c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# submit(test_loader, f'./training_job/2-{IMG_SIZE}-best-checkpoint-040epoch.bin', f'eff_kfold2_{IMG_SIZE}_e40_l005.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2881cadd-9bb6-431b-b557-cc9108a8bde2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test_dataset = DatasetRetriever(\n",
    "# #                         image_ids=train_df.image_id,\n",
    "# #                         marking=train_df,\n",
    "# #                         transforms=get_train_transforms(),\n",
    "# #                         # transforms=get_valid_transforms(),\n",
    "# #                         test=False,\n",
    "# #                         )\n",
    "\n",
    "# # test_loader = torch.utils.data.DataLoader(\n",
    "# #     test_dataset, \n",
    "# #     batch_size=train_config.batch_size,\n",
    "# #     num_workers=0, #train_config.num_workers,\n",
    "# #     shuffle=True,\n",
    "# #     # sampler=SequentialSampler(test_dataset),\n",
    "# #     # pin_memory=False,\n",
    "# #     collate_fn=collate_fn,\n",
    "# # )\n",
    "\n",
    "# # images, targets, image_ids = next(iter(test_loader))\n",
    "# # rst = next(iter(train_loader))\n",
    "\n",
    "# # TRAIN_ROOT_PATH = '../input/vinbigdata-original-image-dataset/vinbigdata/train'\n",
    "# TRAIN_ROOT_PATH = './dataset/'\n",
    "\n",
    "# class DatasetRetriever(Dataset):\n",
    "#     def __init__(self, marking, image_ids, transforms=None, test=False):\n",
    "#         super().__init__()\n",
    "#         self.image_ids = image_ids\n",
    "#         self.marking = marking\n",
    "#         self.transforms = transforms\n",
    "#         self.test = test\n",
    "\n",
    "#     def __getitem__(self, index: int):\n",
    "#         image_id = self.image_ids[index]\n",
    "        \n",
    "#         # image, boxes, labels = self.load_image_and_boxes(index)\n",
    "#         # print(type(image), type(boxes), type(labels), random.random())\n",
    "\n",
    "#         image, boxes, labels = self.load_cutmix_image_and_boxes(index)\n",
    "#         # print(type(image), type(boxes), type(labels))\n",
    "        \n",
    "#         # if self.test or random.random() > 0.33:\n",
    "#         #     image, boxes, labels = self.load_image_and_boxes(index)\n",
    "#         # elif random.random() > 0.5:\n",
    "#         #     image, boxes, labels = self.load_cutmix_image_and_boxes(index)\n",
    "#         # else:\n",
    "#         #     image, boxes, labels = self.load_mixup_image_and_boxes(index)\n",
    "        \n",
    "#         ## To prevent ValueError: y_max is less than or equal to y_min for bbox from albumentations bbox_utils\n",
    "#         labels = np.array(labels, dtype=np.int).reshape(len(labels), 1)\n",
    "#         combined = np.hstack((boxes.astype(np.int), labels))\n",
    "#         combined = combined[np.logical_and(combined[:,2] > combined[:,0], combined[:,3] > combined[:,1])]\n",
    "#         boxes = combined[:, :4]\n",
    "#         labels = combined[:, 4].tolist()\n",
    "        \n",
    "#         target = {}\n",
    "#         target['boxes'] = boxes\n",
    "#         target['labels'] = torch.tensor(labels)\n",
    "#         target['image_id'] = torch.tensor([index])\n",
    "#         if self.transforms:\n",
    "#             for i in range(10):\n",
    "#                 sample = self.transforms(**{\n",
    "#                     'image': image, 'bboxes': target['boxes'], 'labels': labels\n",
    "#                 })\n",
    "#                 print('size:', len(sample['bboxes']))\n",
    "#                 if len(sample['bboxes']) > 0:\n",
    "#                     image = sample['image']\n",
    "#                     target['boxes'] = torch.stack(tuple(map(torch.tensor, zip(*sample['bboxes'])))).permute(1, 0)\n",
    "#                     target['boxes'][:,[0,1,2,3]] = target['boxes'][:,[1,0,3,2]]  ## ymin, xmin, ymax, xmax\n",
    "#                     break\n",
    "            \n",
    "#             if len(target['boxes'])==0 or i==9:\n",
    "#                 return None\n",
    "#             else:\n",
    "#                 try:\n",
    "#                     assert torch.is_tensor(image), f\"Invalid image type:{type(image)}\"\n",
    "#                     assert torch.is_tensor(target['boxes']), f\"Invalid target type:{type(target['boxes'])}\"\n",
    "#                 except Exception as E:\n",
    "#                     print(\"Image skipped:\", E)\n",
    "#                     return None      \n",
    "\n",
    "#         return image, target, image_id\n",
    "\n",
    "#     def __len__(self) -> int:\n",
    "#         return self.image_ids.shape[0]\n",
    "    \n",
    "#     def load_image_and_boxes(self, index):\n",
    "#         image_id = self.image_ids[index]\n",
    "#         image = cv2.imread(f'{TRAIN_ROOT_PATH}{image_id}', cv2.IMREAD_COLOR).copy()\n",
    "#         image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
    "#         image /= 255.0\n",
    "#         records = self.marking[self.marking['image_id'] == image_id]\n",
    "#         boxes = records[['x_min', 'y_min', 'x_max', 'y_max']].values\n",
    "#         labels = records['class_id'].tolist()\n",
    "#         resize_transform = A.Compose(\n",
    "#                                     [A.Resize(height=IMG_SIZE, width=IMG_SIZE, p=1.0)], \n",
    "#                                     # [A.Resize(height=512, width=512, p=1.0)], \n",
    "#                                     p=1.0, \n",
    "#                                     bbox_params=A.BboxParams(\n",
    "#                                         format='pascal_voc', min_area=0.1, min_visibility=0.1, label_fields=['labels'])\n",
    "#                                     )\n",
    "\n",
    "#         resized = resize_transform(**{'image': image, 'bboxes': boxes, 'labels': labels })\n",
    "\n",
    "#         resized_bboxes = np.vstack((list(bx) for bx in resized['bboxes']))\n",
    "#         return resized['image'], resized_bboxes, resized['labels']\n",
    "\n",
    "#     # def load_cutmix_image_and_boxes(self, index, imsize=512):\n",
    "#     def load_cutmix_image_and_boxes(self, index, imsize=IMG_SIZE):\n",
    "#         w, h = imsize, imsize\n",
    "#         s = imsize // 2\n",
    "#         xc, yc = [int(random.uniform(imsize * 0.25, imsize * 0.75)) for _ in range(2)]  # center x, y\n",
    "#         indexes = [index] + [random.randint(0, self.image_ids.shape[0] - 1) for _ in range(3)]\n",
    "#         result_image = np.full((imsize, imsize, 3), 1, dtype=np.float32)\n",
    "#         result_boxes = []\n",
    "#         result_labels = np.array([], dtype=np.int)\n",
    "\n",
    "#         for i, index in enumerate(indexes):\n",
    "#             image, boxes, labels = self.load_image_and_boxes(index)\n",
    "#             if i == 0:\n",
    "#                 x1a, y1a, x2a, y2a = max(xc - w, 0), max(yc - h, 0), xc, yc  # xmin, ymin, xmax, ymax (large image)\n",
    "#                 x1b, y1b, x2b, y2b = w - (x2a - x1a), h - (y2a - y1a), w, h  # xmin, ymin, xmax, ymax (small image)\n",
    "#             elif i == 1:  # top right\n",
    "#                 x1a, y1a, x2a, y2a = xc, max(yc - h, 0), min(xc + w, s * 2), yc\n",
    "#                 x1b, y1b, x2b, y2b = 0, h - (y2a - y1a), min(w, x2a - x1a), h\n",
    "#             elif i == 2:  # bottom left\n",
    "#                 x1a, y1a, x2a, y2a = max(xc - w, 0), yc, xc, min(s * 2, yc + h)\n",
    "#                 x1b, y1b, x2b, y2b = w - (x2a - x1a), 0, max(xc, w), min(y2a - y1a, h)\n",
    "#             elif i == 3:  # bottom right\n",
    "#                 x1a, y1a, x2a, y2a = xc, yc, min(xc + w, s * 2), min(s * 2, yc + h)\n",
    "#                 x1b, y1b, x2b, y2b = 0, 0, min(w, x2a - x1a), min(y2a - y1a, h)\n",
    "#             result_image[y1a:y2a, x1a:x2a] = image[y1b:y2b, x1b:x2b]\n",
    "#             padw = x1a - x1b\n",
    "#             padh = y1a - y1b\n",
    "#             boxes[:, 0] += padw\n",
    "#             boxes[:, 1] += padh\n",
    "#             boxes[:, 2] += padw\n",
    "#             boxes[:, 3] += padh\n",
    "#             result_boxes.append(boxes)\n",
    "#             result_labels = np.concatenate((result_labels, labels))\n",
    "\n",
    "#         result_boxes = np.concatenate(result_boxes, 0)\n",
    "#         np.clip(result_boxes[:, 0:], 0, 2 * s, out=result_boxes[:, 0:])\n",
    "#         result_boxes = result_boxes.astype(np.int32)\n",
    "#         index_to_use = np.where((result_boxes[:,2]-result_boxes[:,0])*(result_boxes[:,3]-result_boxes[:,1]) > 0)\n",
    "#         result_boxes = result_boxes[index_to_use]\n",
    "#         result_labels = result_labels[index_to_use]\n",
    "#         return result_image, result_boxes, result_labels\n",
    "    \n",
    "# # train_dataset = DatasetRetriever(image_ids=train_ids, marking=df_annotations_wbf, transforms=get_train_transforms(), test=False)\n",
    "# # next(iter(train_dataset))\n",
    "\n",
    "# train_dataset = DatasetRetriever(\n",
    "#                     image_ids=train_ids,\n",
    "#                     marking=df_annotations_wbf,\n",
    "#                     transforms=get_train_transforms(),\n",
    "#                     test=False,\n",
    "#                     )\n",
    "\n",
    "# train_loader = torch.utils.data.DataLoader(\n",
    "#     train_dataset,\n",
    "#     batch_size=1, #train_config.batch_size,\n",
    "#     # sampler=RandomSampler(train_dataset),\n",
    "#     shuffle=True,\n",
    "#     pin_memory=False,\n",
    "#     drop_last=True,\n",
    "#     num_workers=train_config.num_workers,\n",
    "#     collate_fn=collate_fn,\n",
    "# )\n",
    "\n",
    "# for images, targets, image_ids in train_loader:\n",
    "#     print('rst:', images)\n",
    "#     break\n",
    "#     # print(images[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f829043d-d01d-426f-8008-a6e68832ffec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# target = targets[0]\n",
    "# img = images[0]\n",
    "\n",
    "# # print(target)\n",
    "# img = img.permute(2,1,0).numpy().copy()\n",
    "# # plt.imshow(img)\n",
    "# # plt.show()\n",
    "\n",
    "# # img = img_input.numpy().transpose((1, 2, 0)).copy()\n",
    "# # print(img.shape)\n",
    "\n",
    "# for idx, box in enumerate(target['boxes']):\n",
    "#     lbl = target['labels'][idx].tolist()\n",
    "#     output = box.numpy().astype(int).tolist()\n",
    "#     # lbl = target['labels'].numpy().tolist()[0]\n",
    "#     output.append(1)\n",
    "#     output.append(lbl)\n",
    "#     # print(output)\n",
    "#     showBox([output], img) \n",
    "\n",
    "# # outputs = target['boxes'].numpy().astype(int).tolist()[0].copy()\n",
    "# # lbl = target['labels'].numpy().tolist()[0]\n",
    "# # # print(box, lbl, type(box))\n",
    "# # outputs.append(1)\n",
    "# # outputs.append(lbl)\n",
    "# # print([outputs])\n",
    "# # showBox([outputs], img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd0b029f-e84a-449b-88b0-9a442860c58a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca49fb3-0729-4682-a7b2-582e7f20475c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
